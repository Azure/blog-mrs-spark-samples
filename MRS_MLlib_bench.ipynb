{"nbformat_minor": 0, "cells": [{"source": "# Configuration\nKeys from:\n* http://spark.apache.org/docs/latest/configuration.html\n* http://spark.apache.org/docs/latest/running-on-yarn.html\n\nWe edited the following configurations to illustrate a different tuning methodology to the one used in MRS's RxSpark.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f \n{\n    \"conf\": {\n        \"spark.jars.packages\": \"com.databricks:spark-csv_2.10:1.4.0\",\n        \"spark.executor.memory\": \"2g\",\n        \"spark.driver.memory\": \"4g\",\n        \"spark.driver.cores\": \"2\",\n        \"spark.executor.cores\": \"2\",\n        \"spark.executor.instances\": \"32\"\n    }\n} ", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark', u'conf': {u'spark.executor.cores': u'2', u'spark.driver.cores': u'2', u'spark.jars.packages': u'com.databricks:spark-csv_2.10:1.4.0', u'spark.driver.memory': u'4g', u'spark.executor.instances': u'32', u'spark.executor.memory': u'2g'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1466578436859_0003</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr><tr><td>36</td><td>application_1467161110841_0045</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr><tr><td>46</td><td>None</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "# Information", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "%%info", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark', u'conf': {u'spark.executor.cores': u'2', u'spark.driver.cores': u'2', u'spark.jars.packages': u'com.databricks:spark-csv_2.10:1.4.0', u'spark.driver.memory': u'4g', u'spark.executor.instances': u'32', u'spark.executor.memory': u'2g'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1466578436859_0003</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr><tr><td>36</td><td>application_1467161110841_0045</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr><tr><td>46</td><td>None</td><td>pyspark</td><td>dead</td><td></td><td></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "# Code", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import time\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql import Row\nfrom pyspark.mllib.tree import DecisionTree\n\n# prefix = \"wasb://nyctaxi@maxkazstorage.blob.core.windows.net/\"\n# single blob files (not a good idea)\n#trainFile = \"wasb://nyctaxi@maxkazsouthcentralus.blob.core.windows.net/trainDump.xdf/trainDump.xdf\"\n#testFile = \"wasb://nyctaxi@maxkazsouthcentralus.blob.core.windows.net/testDump.xdf/testDump.xdf\"\n# split files\ntrainFile = \"wasb://nyctaxi@maxkazstorage.blob.core.windows.net/trainDumpSplitcsv\"\ntestFile = \"wasb://nyctaxi@maxkazstorage.blob.core.windows.net/testDumpSplitcsv\"\ntest  = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load( testFile).drop(\"payment_type\").drop(\"tip_amount\")\ntrain = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(trainFile).drop(\"payment_type\").drop(\"tip_amount\")\n\n# obtain categories for x and recode them to double\ncategories = train.select(\"vendor_id\").distinct().collect()\nvendorMap = {}\nfor x in enumerate(categories):\n    vendorMap[x[1][0]] = float(x[0])\n\ntrainLabeled = train.map(lambda x: LabeledPoint(float(x[-1]), Row(vendorMap[x[0]]) + x[1:-2]))\ntestLabeled  =  test.map(lambda x: LabeledPoint(float(x[-1]), Row(vendorMap[x[0]]) + x[1:-2]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\nCreating HiveContext as 'sqlContext'\n"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "start = time.time()\nmodel = DecisionTree.trainClassifier(trainLabeled, numClasses=2, categoricalFeaturesInfo={0:2}, impurity='gini', maxDepth=10, maxBins=32)\nend = time.time()\nprint \"Time to train on uncached data: %.2f mins\" % ((end - start)/60)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Time to train on uncached data: 6.09 mins"}], "metadata": {"collapsed": false}}, {"source": "# Compute Benchmarks", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "predictions = model.predict(testLabeled.map(lambda x: x.features)).collect()\ntruth = testLabeled.map(lambda x: x.label).collect()\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "from sklearn.metrics import f1_score\nprint f1_score(truth, predictions)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.632199582112"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# load MRS predictions\ntestFile = \"wasb://nyctaxi@maxkazstorage.blob.core.windows.net/predictSplitcsv\"\npredict = [x[0] for x in sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load( testFile).select(\"tipped_Pred\").collect()]", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "from sklearn.metrics import f1_score\nprint f1_score(truth, predict)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.63625822304"}], "metadata": {"collapsed": false}}, {"source": "# Sanity Check\nMake sure we are predicting and testing on the same set as MRS.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "testFile = \"wasb://nyctaxi@maxkazstorage.blob.core.windows.net/predictSplitcsv\"\ntt = [x[0] for x in sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load( testFile).select(\"tipped\").collect()]", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "print tt == truth", "outputs": [{"output_type": "stream", "name": "stdout", "text": "True"}], "metadata": {"collapsed": false}}, {"source": "# Other Benchmarks", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "start = time.time()\ntrainLabeled.cache()\ntrainLabeled.count()\nend = time.time()\nprint \"Time to cache and materialize training data: %.2f mins\" % ((end - start)/60)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Time to cache and materialize training data: 4.45 mins"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "start = time.time()\nmodel = DecisionTree.trainClassifier(trainLabeled, numClasses=2, categoricalFeaturesInfo={0:2}, impurity='gini', maxDepth=10, maxBins=32)\nend = time.time()\ntrainLabeled.unpersist()\nprint \"Time to train on cached data: %.2f mins\" % ((end - start)/60)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Time to train on cached data: 6.69 mins"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}, "celltoolbar": "Edit Metadata"}}